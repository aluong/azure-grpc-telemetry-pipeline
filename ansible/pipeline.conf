##############################################
# Global bits of config in the default section
#
[default]

#
# ID is used to identify pipeline to third parties, e.g. fluentd tag
# and prometheus instance
id = pipeline

##############################################
# Example of a gRPC dialout (router connects to pipeline over gRPC)
#[gRPCDialout]
#
# stage = xport_input
#
# grpc: in the role of server with router connecting to pipeline and
# client side streaming.
#
# type = grpc
#
# encap = gpb
# Encapsulation pushed by client: gpb, gpbkv, gpbcompact.
# As of 6.1.1 release, we support gpb (which is a common
# header used to carry compact and k/v gpb), and we default
# to gpb. In older releases, it might be necessary to configure
# compact or k/v.
#
# Socket to listen on
#
# listen = :57500
#
# Access control options; TLS
#
#
# To enable dumping data as it is rxed, uncomment the following, and
# run with --debug option.
#
# logdata = on
#

[gRPCDialout]
stage = xport_input
type = grpc
encap = gpb
listen = :443

# TLS enabled or not, server CA cert in PEM, and server name cert
# issued for. (Look for CN in 'openssl x509 -noout -text -in <ca.pem>')
#
tls = true
tls_pem = etc/pipeline/pipeline_vm_cert.pem
tls_key = /etc/pipeline/pipeline_vm_key.pem
tls_servername = <TLS_SERVERNAME>

##############################################
# Example of a kafka output stage: used to publish content to kafka
#
# [mykafka]
# stage = xport_output
#
# Module type: kafka publisher is only supported in xport_output stage currently.
#
# type = kafka
#
# Kafka specific key option. This is an optional setting and is
# specific to a weird requirement of our home grown consumer on kafka
# bus.
#
# key = id
#
# Encoding: gpb, gpbkv, json or json_events
#
# encoding = json
#
# Kafka specific option. The brokers for the kafka bus
#
# brokers = kafka.example.com:9092
#
# Kafka specific option. The topic to publish against.
#
# topic = telemetry
#
# Optional: It is also possible to specify a dynamic derivation of
# topic to send on. This mechanism is a text template applied to the
# metadata of the message; currently a structure containing the Path
# (encoding path) and Identifier (router name) fields. In future the
# whole message body may be exposed. In the case where the template
# fails to extract and return a string, the message will be published
# on 'topic' above.
#
# The example extract encoding path:
#
# topic_metadata_template = topic_template_testb.txt
#
# The option required_acks can be used to influence whether the
# producer waits for acknowledgments from just the local broker
# (i.e. the broker to which the message is published), or for a
# consensus commit from replicas. The options are "local" and "commit"
# respectively. By default, we default to "none".
#
# required_acks = none
#
# The optional buffered channel depth used to accommodate transient
# producer/consumer throughput.
#
# datachanneldepth = 1000
#
# To enable dumping data as it is rxed, uncomment the following, and
# run with --debug option.
#
# logdata = on

[mykafka]
type = kafka
encoding = json
stage = xport_output
brokers = <EH_NAMESPACE>.servicebus.windows.net:9093
topic = raw
tls = on
sasl = on
saslUsername = $ConnectionString
saslPassword = <CONNSTRING>
kafkaVersion = 1.1.0
required_acks = commit
